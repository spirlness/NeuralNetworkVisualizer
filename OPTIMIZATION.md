# ä»£ç ä¼˜åŒ–å»ºè®®æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: 2026-01-22  
**é¡¹ç›®**: Neural Network Visualizer  
**æ‰«æèŒƒå›´**: å…¨éƒ¨æºä»£ç  (28 æ–‡ä»¶, ~3100 è¡Œ)

æœ¬æŠ¥å‘Šæ±‡æ€»äº† 4 ä¸ªå¹¶è¡Œåå°æ™ºèƒ½ä½“çš„æ‰«æç»“æœï¼Œæä¾›äº†åˆ†çº§ä¼˜åŒ–å»ºè®®ï¼ˆé«˜/ä¸­/ä½å½±å“ï¼‰ã€‚

---

## ğŸ“Š æ‰«æç»“æœæ±‡æ€»

| æ‰«æç±»å‹ | å‘ç°é—®é¢˜æ•° | ä¼˜å…ˆçº§åˆ†å¸ƒ |
|----------|------------|-----------|
| **æ€§èƒ½ä¼˜åŒ–** | 10 ä¸ªç“¶é¢ˆ | é«˜:4, ä¸­:3, ä½:3 |
| **ä»£ç é‡å¤** | 4 å¤§ç±» | é‡æ„æœºä¼š:5 |
| **ç°ä»£C++ç‰¹æ€§** | 7 ä¸ªæ”¹è¿›ç‚¹ | å¯è¯»æ€§/å®‰å…¨æ€§æå‡ |
| **Qtæœ€ä½³å®è·µ** | 3 ä¸ªé—®é¢˜ | constæ­£ç¡®æ€§, çº¿ç¨‹å®‰å…¨ |

---

## ğŸ”¥ é«˜å½±å“ä¼˜åŒ–ï¼ˆæ¶æ„çº§ï¼‰

### 1. **Cache-Unfriendly æƒé‡å¸ƒå±€** 
**å½±å“**: ğŸ”´ é«˜ - MLP æ€§èƒ½æ ¸å¿ƒç“¶é¢ˆ  
**ä½ç½®**: `neural_network.h:21`, `neural_network.cpp`  
**é—®é¢˜**: 
```cpp
// å½“å‰ï¼ˆç¢ç‰‡åŒ–å†…å­˜ï¼‰
std::vector<std::vector<double>> weights;  // æ¯è¡Œç‹¬ç«‹åˆ†é…
```
**ä¿®å¤**:
```cpp
// æ¨èï¼ˆè¿ç»­å†…å­˜ï¼‰
std::vector<double> weights;  // æ‰å¹³åŒ–: inputSize * outputSize
double& w(int i, int j) { return weights[i * outputSize + j]; }
```
**é¢„æœŸæ”¶ç›Š**: 20-40% æ€§èƒ½æå‡ï¼ˆç¼“å­˜å‘½ä¸­ç‡å¤§å¹…æ”¹å–„ï¼‰

---

### 2. **å¤§é‡ Tensor/Vector æ‹·è´**
**å½±å“**: ğŸ”´ é«˜ - CNN è®­ç»ƒé€Ÿåº¦æ ¸å¿ƒç“¶é¢ˆ  
**ä½ç½®**: `cnn_network.cpp:125-128`, `neural_network.cpp:103,116`  
**é—®é¢˜**: 
```cpp
// å½“å‰ï¼ˆæ¯å±‚éƒ½å¤åˆ¶ï¼‰
Tensor output = layer->forward(input);  // å€¼ä¼ é€’
std::vector<double> currentInput = input;  // æ¯æ¬¡å¤åˆ¶
```
**ä¿®å¤**:
```cpp
// æ¨èï¼ˆPing-Pong ç¼“å†²ï¼‰
Tensor bufferA, bufferB;
Tensor* current = &bufferA;
Tensor* next = &bufferB;
for (auto& layer : layers) {
    layer->forward(*current, *next);  // å¼•ç”¨ä¼ é€’
    std::swap(current, next);
}
```
**é¢„æœŸæ”¶ç›Š**: 30-50% æ€§èƒ½æå‡ï¼ˆæ¶ˆé™¤å†…å­˜åˆ†é…ï¼‰

---

### 3. **Naive å·ç§¯å®ç°**
**å½±å“**: ğŸ”´ é«˜ - CNN æœ€å¤§ç“¶é¢ˆ  
**ä½ç½®**: `conv_layer.cpp:92-113, 148-180`  
**é—®é¢˜**: 6-7 å±‚åµŒå¥—å¾ªç¯  
**ä¿®å¤**: ä½¿ç”¨ **im2col + GEMM** ç®—æ³•
```cpp
// å½“å‰: O(C_out * C_in * H * W * K * K)
for (c_out) 
  for (c_in) 
    for (oh) 
      for (ow) 
        for (kh) 
          for (kw) // 7å±‚åµŒå¥—ï¼

// æ¨è: im2col è½¬æ¢ä¸ºçŸ©é˜µä¹˜æ³•
Matrix col = im2col(input);
Matrix result = kernels * col;  // BLAS ä¼˜åŒ–
```
**é¢„æœŸæ”¶ç›Š**: 5-10x æ€§èƒ½æå‡ï¼ˆåˆ©ç”¨ BLASï¼‰

---

### 4. **é‡å¤è®¡ç®—åŠ æƒå’Œ**
**å½±å“**: ğŸ”´ é«˜ - å†—ä½™è®¡ç®—  
**ä½ç½®**: `neural_network.cpp:129,147`, `cnn_network.cpp:175`  
**é—®é¢˜**: åå‘ä¼ æ’­é‡æ–°è®¡ç®—å‰å‘å·²ç®—è¿‡çš„ z = wx+b  
**ä¿®å¤**:
```cpp
// å‰å‘ä¼ æ’­æ—¶å­˜å‚¨
layer.preActivation[j] = sum;  // ä¿å­˜ z
layer.output[j] = activate(sum);

// åå‘ä¼ æ’­ç›´æ¥ä½¿ç”¨
derivative = activateDerivative(layer.preActivation[j]);
```
**é¢„æœŸæ”¶ç›Š**: 15-25% æ€§èƒ½æå‡

---

## ğŸŸ¡ ä¸­ç­‰å½±å“ä¼˜åŒ–ï¼ˆé€»è¾‘çº§ï¼‰

### 5. **Padding é¢‘ç¹åˆ†é…**
**å½±å“**: ğŸŸ¡ ä¸­  
**ä½ç½®**: `conv_layer.cpp:84-87`  
**é—®é¢˜**: æ¯æ¬¡å‰å‘/åå‘éƒ½åˆ›å»ºæ–°çš„ padded Tensor  
**ä¿®å¤**: é¢„åˆ†é… padded ç¼“å†²åŒºæˆ–ç›´æ¥åœ¨å¾ªç¯ä¸­å¤„ç†åç§»

---

### 6. **æ¿€æ´»å‡½æ•°å†—ä½™è°ƒç”¨**
**å½±å“**: ğŸŸ¡ ä¸­  
**ä½ç½®**: `neural_network.cpp:83-86`  
**é—®é¢˜**:
```cpp
// å½“å‰ï¼ˆSigmoid è°ƒä¸¤æ¬¡ï¼‰
double activateDerivative(double x, ActivationType::Sigmoid) {
    double s = activate(x, Sigmoid);  // exp() è°ƒç”¨
    return s * (1.0 - s);
}
```
**ä¿®å¤**:
```cpp
// åˆ©ç”¨ f'(z) = f(z)(1-f(z))ï¼Œf(z) å·²å­˜å‚¨
double derivative = output * (1.0 - output);
```

---

### 7. **Tensor ç´¢å¼•è®¡ç®—å¼€é”€**
**å½±å“**: ğŸŸ¡ ä¸­  
**ä½ç½®**: `tensor.cpp`, `tensor.h:96-98`  
**é—®é¢˜**: `index(c,h,w)` åœ¨ç´§å¯†å¾ªç¯ä¸­é‡å¤ä¹˜æ³•  
**ä¿®å¤**: ä½¿ç”¨æŒ‡é’ˆè¿ç®—æˆ–è¿­ä»£å™¨

---

## ğŸŸ¢ ä½å½±å“ä¼˜åŒ–ï¼ˆå¾®ä¼˜åŒ–ï¼‰

### 8. **RNG é‡å¤åˆå§‹åŒ–**
**ä½ç½®**: `neural_network.cpp:21-22`, `tensor.cpp:66,78`  
**ä¿®å¤**: ä½¿ç”¨å•ä¾‹æˆ–æˆå‘˜å˜é‡çš„ `std::mt19937`

---

### 9. **ç¼ºå°‘ reserve()**
**ä½ç½®**: `neural_network.cpp:197-203`  
**ä¿®å¤**: `sizes.reserve(layers_.size() + 1)`

---

### 10. **ç´§å¯†å¾ªç¯ä¸­çš„åˆ†æ”¯**
**ä½ç½®**: `conv_layer.cpp:172`  
**ä¿®å¤**: åˆ†ç¦»è¾¹ç•Œå’Œä¸­å¿ƒå¾ªç¯

---

## ğŸ”„ ä»£ç é‡æ„æœºä¼š

### é‡å¤é€»è¾‘é›†ä¸­åŒ–

| é‡å¤ç±»å‹ | å‡ºç°ä½ç½® | å»ºè®®è§£å†³æ–¹æ¡ˆ |
|----------|----------|-------------|
| **æ¿€æ´»å‡½æ•°** | `neural_network.cpp`, `conv_layer.cpp`, `cnn_network.cpp` | åˆ›å»º `utils/activations.h` å·¥å…·ç±» |
| **æƒé‡åˆå§‹åŒ–** | `Layer::initializeWeights`, `Tensor::xavierInit/heInit` | åˆ›å»º `utils/weight_init.h` ç»Ÿä¸€æ¥å£ |
| **Dense å±‚é€»è¾‘** | `NeuralNetwork::forward/backward`, `CNNNetwork` æ‰‹åŠ¨å®ç° | é‡æ„ä¸º `DenseLayer` ç±»ï¼Œä¸¤è€…å…±äº« |
| **å¯è§†åŒ–å½’ä¸€åŒ–** | `NetworkView`, `FeatureMapView` | åˆ›å»º `utils/vis_utils.h` å…±äº«è‰²å½©æ˜ å°„ |
| **è®­ç»ƒå¾ªç¯** | `TrainingThread` (QThread), `CNNMainWindow` (QTimer) | ç»Ÿä¸€ä¸º `TrainingController` |

---

## âš¡ ç°ä»£ C++ ç‰¹æ€§åº”ç”¨

### C++17 ç‰¹æ€§

| ç‰¹æ€§ | åº”ç”¨ä½ç½® | ç¤ºä¾‹ |
|------|----------|------|
| **ç»“æ„åŒ–ç»‘å®š** | `loss_chart.cpp:25,125` | `for (const auto& [epoch, loss] : dataPoints_)` |
| **if-init** | `main.cpp:30-31` | `if (const auto args = parser.positionalArguments(); !args.isEmpty())` |
| **string_view** | `cnn_layer_base.h:66` | `virtual std::string_view name() const = 0;` |
| **[[nodiscard]]** | `tensor.h`, `neural_network.h` | `[[nodiscard]] double mean() const;` |
| **constexpr** | `neural_network.cpp:71` | `static constexpr double ActivationLimit = 500.0;` |

### C++20 ç‰¹æ€§ï¼ˆå¯é€‰ï¼‰

- `std::span<const double>` æ›¿ä»£ `const std::vector<double>&`
- `std::numbers::pi` æ›¿ä»£ `M_PI`

---

## ğŸ¨ Qt æœ€ä½³å®è·µæ”¹è¿›

### Const æ­£ç¡®æ€§ï¼ˆç¼ºå¤±ï¼‰

| æ–¹æ³• | æ–‡ä»¶ | ä¿®å¤ |
|------|------|------|
| `CNNView::getLayerColor` | `cnn_view.cpp` | æ·»åŠ  `const` |
| `FeatureMapView::valueToColor` | `feature_map_view.cpp` | æ·»åŠ  `const` |
| `CNNNetwork::getAllFeatureMaps` | `cnn_network.h` | æ·»åŠ  `const` |

### çº¿ç¨‹å®‰å…¨

**é—®é¢˜**: CNN è®­ç»ƒåœ¨ä¸»çº¿ç¨‹ï¼ˆQTimerï¼‰  
**è§£å†³**: åˆ›å»º `CNNTrainingThread` ç±»ä¼¼ MLP  
**ä¼˜å…ˆçº§**: ğŸ”´ é«˜ï¼ˆç”¨æˆ·ä½“éªŒï¼‰

---

## ğŸ“ˆ ä¼˜åŒ–ä¼˜å…ˆçº§å»ºè®®

### ç«‹å³å®æ–½ï¼ˆé«˜ROIï¼‰

1. âœ… **å·²ä¿®å¤**: Include è·¯å¾„ã€æ•°å€¼ç¨³å®šæ€§ã€CLI å‚æ•°
2. ğŸ”´ **æ¨è**: Tensor Ping-Pong ç¼“å†²ï¼ˆ30-50% æå‡ï¼Œå·¥ä½œé‡ä¸­ç­‰ï¼‰
3. ğŸ”´ **æ¨è**: æ¿€æ´»å‡½æ•°é‡å¤è°ƒç”¨ä¿®å¤ï¼ˆ15-25% æå‡ï¼Œå·¥ä½œé‡ä½ï¼‰
4. ğŸ”´ **æ¨è**: CNN è®­ç»ƒçº¿ç¨‹åŒ–ï¼ˆç”¨æˆ·ä½“éªŒå…³é”®ï¼‰

### ä¸­æœŸè§„åˆ’ï¼ˆé‡æ„ï¼‰

5. ğŸŸ¡ æå–æ¿€æ´»å‡½æ•°/æƒé‡åˆå§‹åŒ–å·¥å…·ç±»
6. ğŸŸ¡ ç»Ÿä¸€ Dense å±‚å®ç°
7. ğŸŸ¡ åº”ç”¨ç°ä»£ C++ ç‰¹æ€§ï¼ˆå¯è¯»æ€§ï¼‰

### é•¿æœŸä¼˜åŒ–ï¼ˆé«˜çº§ï¼‰

8. ğŸŸ¢ æ‰å¹³åŒ–æƒé‡çŸ©é˜µï¼ˆéœ€è¦å¤§é‡æµ‹è¯•ï¼‰
9. ğŸŸ¢ im2col + GEMM å·ç§¯ï¼ˆå¤æ‚å®ç°ï¼‰
10. ğŸŸ¢ é›†æˆ Eigen/OpenBLASï¼ˆå¤–éƒ¨ä¾èµ–ï¼‰

---

## ğŸ“ æ€»ç»“

**å½“å‰çŠ¶æ€**: ä»£ç æ¶æ„æ¸…æ™°ï¼Œæ— ä¸¥é‡ bugï¼Œæ„å»ºæˆåŠŸ  
**æ€§èƒ½æ½œåŠ›**: é€šè¿‡ä¼˜åŒ– 2+3+4 å¯è·å¾— **3-5x æ•´ä½“æ€§èƒ½æå‡**  
**å¯ç»´æŠ¤æ€§**: é‡æ„æœºä¼šæ˜ç¡®ï¼Œä»£ç é‡å¤å¯å‡å°‘ 30-40%  
**ç°ä»£åŒ–**: å¯é€æ­¥åº”ç”¨ C++17/20 ç‰¹æ€§æå‡ä»£ç è´¨é‡

**å»ºè®®è·¯å¾„**:  
Phase 1: ä¿®å¤ä½æˆæœ¬é«˜æ”¶ç›Šé¡¹ï¼ˆæ¿€æ´»å‡½æ•°ã€Ping-Pong ç¼“å†²ï¼‰  
Phase 2: ä»£ç é‡æ„ï¼ˆå·¥å…·ç±»æå–ã€Dense å±‚ç»Ÿä¸€ï¼‰  
Phase 3: é«˜çº§ä¼˜åŒ–ï¼ˆim2colã€BLAS é›†æˆï¼‰

---

**æ³¨æ„**: æ‰€æœ‰ä¼˜åŒ–å»ºè®®å‡åŸºäºæ¢ç´¢æ™ºèƒ½ä½“çš„æ·±åº¦ä»£ç åˆ†æï¼Œæœªå®é™…æ€§èƒ½æµ‹è¯•ã€‚å®æ–½å‰åº”å…ˆå»ºç«‹åŸºå‡†æµ‹è¯•ã€‚
